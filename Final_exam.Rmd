---
output:
  pdf_document: default
  html_document: default
---
# Instructions
You're almost done with the semester! Take a second to congratulate yourself on getting here. As a reminder, this final project is simply an (imperfect) way of measuring what you have learned throughout the semester. So take a deep breath and do your best, but also remember that it doesn't determine your value as a human being.

The exam is split into 4 sections: Module 1, 2 and 3 (6 questions), Modules 4 and 5 (3 questions), Module 6 (2 questions) and the final project. Most of the questions on this exam are short answers. You don't need to write out an overly long response (a sentence or so for each part of the question should be fine), but you should be specific in explaining your response. For example, if there is a question about whether the assumptions are reasonable. You shouldn't just say "from the plot we can see that the linearity assumption is (or is not) reasonable," but instead you should explain specifically why the plot leads you to believe the linearity assumption is (or is not) reasonable.

The exam is open notes so you **can** use any of the material or any of the notes you have taken throughout the class. You **cannot** discuss the exam (while it is in progress) with anyone else. You also **cannot** use any generative AI tools. Submissions will be sent by e-mail to **nbb45@cornell.edu** before **May 14th 11:59pm**.    

\newpage

# Module 1, 2, and 3
In the questions for Modules 1, 2, and 3, we will look at data from SNCF, France's national railway. The data has been cleaned and made easily available by [TidyTuesday](https://github.com/rfordatascience/tidytuesday/tree/master/data/2019/2019-02-26). In particular, we have data on train delays from each month between 2015-2018 for each train route (i.e., from city A to city B). So each observation (i.e., row in the data) corresponds to a specific route in a specific year and month. In the dataset, we will be particularly interested in the following variables

For each row in the data, we have the following variables

* year : year of observation (2015, 2016, 2017 or 2018)
* month : month of observation (1, 2, ..., 12)
* departure_station : station where the route begins (e.g., "PARIS NORD" or "MONTPELLIER")
* arrival_station : station where the route ends (e.g., "PARIS NORD" or "MONTPELLIER")
* journey_time_avg : average journey time in minutes for the route for that year and month
* avg_delay_all_departing : average delay in minutes  for all departures for the route for that year and month (i.e., how many minutes the train was late to leave departure station)
* avg_delay_all_arriving : average delay in minutes for all arrivals for the route for that year and month (i.e., how many minutes the train was late to arrive at the arrival_station)

In the following questions, the model you fit or consider may change from question to question.


```{r, fig.align='center', fig.height=3}
## Load in data and remove some outliers
train_data <- read.csv("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-02-26/small_trains.csv")
# removing some outliers
train_data <- train_data[-which(train_data$avg_delay_all_arriving < -30),]
train_data <- train_data[-which(train_data$avg_delay_all_departing > 30),]
# make month and year factors
train_data$month <- as.factor(train_data$month)
train_data$year <- as.factor(train_data$year)
```


## Question 1 (2 pts)
Suppose we are interested in modeling the average delayed arrival; i.e., avg_delay_all_arriving is the outcome variable. Specifically, we would like to investigate the association between average delayed arrival and journey time (journey_time_avg) when controlling for the average departure delay (avg_delay_all_departing).

Fit the relevant linear model below and write 1 sentence interpreting the estimated coefficient for journey_time_avg. 

#### Question 1 Answer

```{r}
hist(train_data$avg_delay_all_arriving)
hist(train_data$journey_time_avg)
hist(train_data$avg_delay_all_departing)
mod <- lm(avg_delay_all_arriving ~ journey_time_avg + avg_delay_all_departing, data=train_data)
summary(mod)
```
The estimated coefficient for journey_time_avg is 0.022, which means that for every one minute increase in the length of the journey, there is a 0.022 minute increase in the delay in arrival at the destination when there is no change in the delay in departing. 

## Question 2 (2 pts)
Some output for a **different model** is shown below. Using the output, predict the average arrival delay for a train route which has an average journey time of 200 minutes, has an average departure delay of 3 minutes, and took place in January (i.e., month == 1). 
```{r, echo =F}
mod2 <- lm(avg_delay_all_arriving ~ journey_time_avg + avg_delay_all_departing + month,
           data = train_data)
summary(mod2)$coef
```
#### Question 2 Answer
The MLR finds the solution to the model:
y_hat = b0(month) + b1*x1 + b2*x2 + e
I will solve for the predicted average arrival, ignoring the error term. The month 1 is the reference term so we will just use the intercept
```{r}
y_hat = -0.89153617 + 0.02215535 * 200 + 0.79854766 * 3
y_hat
```
The predicted average delay in arriving is 5.93 minutes.


## Question 3 (6 pts)
Do the assumptions for linear regression seem reasonable for the model fit in Question 2? Explain why or why not? You should use the plots below to justify your answer.
```{r, fig.align='center', fig.height=3, echo = F}
par(mfrow = c(1,2))
plot(mod2$fitted.values, mod2$residuals, pch = 19, cex = .1,
     xlab = "fitted values", ylab = "residuals")
plot(mod2$fitted.values, train_data$avg_delay_all_arriving,
     pch = 19, cex = .1 , xlab = "fitted values", ylab = "observed values")
abline(a = 0, b = 1, col = "red")
```


#### Question 3 Answer
The assumptions to test are really the independence of errors. Here I list the key assumptions and describe my thoughts on them
1. independence of observations - the sampling of the data is independent train trips, so I think this is okay. 
2. independence of errors- here, we se the errors generally fan out in a blob around the data. It looks like there is a slight increase in spread towards the greater fitted values, but I think that the data points affected by this are few enough that it does not violate the assumption. The majority of the weight of the data experiences equal variance of error
3. normality (which I looked at above) - while the distribution is not perfectly symmetric, it looks pretty plausible (smooth, unimodal) and does not violate the assumption



## Question 4 (2 pts)
Suppose you think the association between arrival delay and journey time (i.e., the slope of journey time) may change from year to year. Fit a linear model below which would allow for that. For this problem, you **do not** need to consider adjusting for other variables in the model.


Let's introduce year as an interaction term.
```{r}
mod <- lm(avg_delay_all_arriving ~ journey_time_avg * year,data=train_data)
summary(mod)
```

### Question 4 Answer
Here we see that year had a significant but small impact on the interaction between journey_time_avg and the delay in arrival in 2016 and 2017, but not significant in 2018. The effect of the interaction is much smaller than the effect of year or journey_time_avg independently, so we can conclude that the interaction effect is minimal.


## Question 5 (3 pts)
Below, we fit a model which includes the covariates journey time, average departing delay and month. Suppose we want to test if the average arrival delay is associated with month after adjusting for journey time and average departure delay. For this problem, you don't need to consider interaction terms and you don't need to include other covariates. Describe how you would test this hypothesis. You don’t need to actually perform any calculations or write any code, but specify which function in R you would use and be specific about what the inputs would be.
 
```{r}
mod_year <- lm(avg_delay_all_arriving ~ journey_time_avg + avg_delay_all_departing + month,
               data = train_data)
summary(mod_year)
```
#### Question 5 answer
I would use this model exactly, which controls for journey time and delay in departing and tests the signficance of the impact of month on arrival delay when there is no change in journey time or avg delay in departure.  I would use the model provided, and read the p-values to make my decision as to the association of month. Since all months except 4 have a significant association, I conclude there is a significant effect.

## Question 6 (2 pt)
Suppose we fit the model below where we have used the log of journey_time_avg. Write 1 sentence interpreting the coefficient for journey time.  

```{r}
mod_log <- lm(avg_delay_all_arriving ~ log(journey_time_avg),
              data = train_data)
summary(mod_log)
```
#### Question 6 answer
```{r}
3.29684 * log(1.01)
```

Since we log transform journey time, but not the responce, I interpret this to mean two observationw which differ by 1% journey time have expected avg delay of arriving that differs by 3.28%.

# Module 4 and 5


## Question 7 (3 pts)
In the model you fit in Question 1, each observation in the dataset corresponds to a specific route observed in a specific month and year. Thus each route appears in the data multiple times. Explain why this might violate an assumption for linear regression. How could you fix this? If your suggestion involves additional covariates or a different modeling assumption, be specific about what you mean (i.e., say what covariates would you include, or what model you would fit). There is more than 1 reasonable answer for this question, but just pick one.

#### Question 7 answer

This might violate the assumption of independent observations, since there could be factors specific to each route that affect all observations made on that route, introducing covariance where none is assumed. 

I could fix this by including a random effect for each departure or arrival station - or, perhaps, by creating a new variable composed of departure and arrival station, and including it as a random effect in the model. 
```{r}
train_data$route <- as.factor(paste(train_data$departure_station, train_data$arrival_station))
length(unique(train_data$route))
library(lme4)
model <- lmer(avg_delay_all_arriving ~ journey_time_avg + avg_delay_all_departing + (1|route),
            data=train_data)
summary(model)
coef(model)$route
```

And it looks like there is quite a large degree of variation within these clusters. 

## Question 8 (3 pts)
Using the model from Question 5, we plot the fitted values vs the residuals below. Explain why you might want to use robust standard errors. What might be the advantages and disadvantages of using the robust standard errors as opposed to the model based errors (the ones that come out of \texttt{summary})?

```{r, echo = F}
mod_log <- lm(avg_delay_all_arriving ~ log(journey_time_avg),
              data = train_data)
plot(mod_log$fitted.values, mod_log$residuals, pch = 19, cex = .1)
summary(mod_log)
```

#### Question 8 answer
Here, the error structure shows a banding pattern, as well as a mild fanning pattern (the spread of the residuals increases as the fitted values increase). The violation of the assumption of homoskedasticity can lead to increased type I error rates and incorrect inference procedures (since our standard errors become biased) if we do not account for it.

The disadvantages are that with large sample sizes, hypothesis tests and CIs are valid even with the heteroskedasticity, so this may add compute and complexity unnecessarily. We might also lose power unnecessarily if our model is correct, and if the error in model set up results in bias in parameter estimates, then the robust estimators will not help, since they affect the standard error, not the coefficient estimates.

## Question 9 (3 pts)
Suppose you are taking a train tomorrow from Lille to Paris Nord and want to predict the delay in arrival. You want to be very sure about the prediction, so you gather data for 1000 different variables you think might be relevant (temperature, whether it is raining, GDP of France per month/year, the win/loss record of the soccer team in Lille, etc). You then regress average arrival delay onto all of those variables, and use it to predict the arrival delay for tomorrow's train. Explain why this might not give a good prediction. What might you do instead? 2-3 sentences for this answer is fine.

#### Question 9 answer

Here, I introduce an over-fitting problem. My model might essentially learn the regression on the present variables really well, but not generalize well. What I would do instead is 1. used a penalized regression method like RIDGE or LASSO - with 1k parameters it is likely not all have equal predictive weight, so I would want to limit the impact of some. This would also help with model interprability. I might also try a variable selection method, like forward or backward search. And 2. I would use cross-validation, repeatedly holding out subsets of the data, and measure MSE to determine which model is likely to generalize best. 


# Module 6
For the following questions, suppose we are analyzing data for Big Red Airlines, Cornell's latest idea for getting people to and from Ithaca. The dependent variable is whether or not a flight took off on time. In the \texttt{OnTime} variable: 1 indicates that the flight took off on time, 0 indicates that it was delayed. The covariates we have recorded include Temperature (in degrees), TimeOfDay (Evening, Midday, Morning), and Rain (FALSE, TRUE). 
```{r}
airlineData <- read.csv("https://raw.githubusercontent.com/ysamwang/btry6020_sp22/main/lab11/airline.csv")
names(airlineData)
```

## Question 10 (2 pts)
What is the appropriate type of regression for modeling the binary data? What is being predicted by the linear model we are fitting? i.e., if the model we set up is 
$$ ? = b_0 + b_1 X_{1,i} + b_2 X_{2,i} \ldots$$ what is on the left side of the equation (you can write it out in words instead of typing out the math)?. 

#### Question 10 answer

We are setting up a logistic regression, which aims to predict a 1 or 0 value of wether or not something will occur. It does this by taking the log of the exponential function: log(theta/(1-theta)), where theta is the result of the linear model. We are modeling the log odds of the event happening.


## Question 11 (2 pts)
We fit the model below. How would you interpret the coefficient associated with \texttt{Temperature}?
```{r}
mod <- glm(OnTime ~ Temperature + TimeOfDay + Rain,
           data = airlineData, family = "binomial")
summary(mod)
```
#### Question 11 answer
```{r}
exp(-0.05248)
```

For this regression, the coefficient of temperature indicates that for an observation that only differs by an increase of 1 degree in temperature from another observation, the odds ratio of the second event occurring vs the first event is 0.9488. So there is a slight decrease in plane departure on time probability for an increase in temperature.

\newpage

# Final Project (30 pts)

## Introduction

This final project is designed to demonstrate your mastery of linear regression techniques on real-world data. You will apply the theoretical concepts we've covered in class to a dataset of your choice, perform a comprehensive analysis, and present your findings in a professional format suitable for showcasing to potential employers.

## Objectives

By completing this project, you will:

* Apply linear regression techniques to solve real-world problems
* Demonstrate your ability to verify and address regression assumptions
* Perform meaningful feature selection and hypothesis testing
* Communicate the practical significance of your statistical findings
* Create a professional portfolio piece for future employment opportunities

## Project Requirements

### Dataset Selection

1. Choose a dataset from Kaggle
2. Your dataset must have a continuous target variable suitable for linear regression
3. The dataset should contain multiple potential predictor variables
4. Choose a dataset that interests you and has meaningful real-world applications

### Analysis Requirements
Your analysis must include the following components:

#### Exploratory Data Analysis

* Summary statistics of variables
* Visualization of distributions and relationships
* Identification of missing values and outliers
* Data cleaning and preprocessing steps


#### Regression Assumptions Verification

* Linearity assessment
* Normality of residuals
* Homoscedasticity (constant variance of residuals)
* Independence of observations
* Multicollinearity assessment


#### Assumption Violation Handling

* Apply appropriate transformations when assumptions are violated
* Document your approach to each violation
* Compare models before and after corrections


#### Variable Selection & Hypothesis Testing

* Implement at least two different variable selection techniques
* Perform hypothesis tests on coefficients
* Assess model performance with metrics (R², adjusted R², RMSE, etc.)
* Validate your model using appropriate cross-validation techniques


#### Feature Impact Analysis

* Quantify and interpret the impact of each feature on the target
* Provide confidence intervals for significant coefficients
* Explain the practical significance of your findings in the context of the dataset


#### Deliverables

GitHub Repository containing:

* All code (well-documented Rmd files)
* README.md with clear instructions on how to run your analysis
* Data folder (or instructions for accessing the data)
* Requirements.txt or environment.yml file


#### Final Report (PDF) containing:

* Introduction: dataset description and problem statement
* Methodology: techniques used and justification
* Results: findings from your analysis
* Discussion: interpretation of results and limitations
* Conclusion: summary and potential future work
* References: cite all sources used


## Evaluation Criteria
Your project will be evaluated based on:

* Correctness of statistical analysis and procedures
* Proper handling of regression assumptions
* Quality of variable selection and hypothesis testing
* Clarity of interpretation and insights
* Organization and documentation of code
* Professional presentation of findings

## Timeline and Submission

* Release Date: May 5th, 2025
* Due Date: Wednesday, May 14th, 2025 (11:59 PM EST)
* Submission: Email your GitHub repository link and PDF report to nbb45@cornell.edu with the subject line "Final Project - [Your Name]"

## Resources

* Course materials and lecture notes
* [Kaggle Datasets](https://www.kaggle.com/datasets)
* [GitHub tutorial](https://nayelbettache.github.io/documents/STSCI_6020/Github_tutorial.pdf) and [GitHub documentation](https://docs.github.com/en/repositories) for repository setup.

## Academic Integrity
This is an individual project. While you may discuss general concepts with classmates, all submitted work must be your own. Proper citation is required for any external resources used.

Good luck with your project! This is an opportunity to demonstrate your skills and create a valuable addition to your professional portfolio.



# Finished


You're done, congratulations!

